.. _scraper:

=====================
Police Report Scraper
=====================

The scraper (xmlScraper.py) works on pdf arrest report files that have 
ALREADY BEEN CONVERTED to .xml files via the pdftohtml -xml utility.

To run the program, import the module if necessary and call
	
	getSinglePDFObj(filepath)

if "filepath" refers to a single file, or call

	createPDFList(directorypath) or
	createPDFDict(directorypath)

if "filepath" refers to a directory containing converted .xml files
(and nothing else).

getSinglePDFObj(filepath) returns a python dictionary that contains the 
key-value pairs from one arrest report. The keys of the dictionary are the
names of the fields on the report ("OCA", "Address", "Charge1", etc.).
A list of the keys can be obtained using the built-in python dictionary
function .keys(), or alternatively you can print a nicely readable version
of the keys and their values by passing the dict as a parameter to the
function printPDFDict(pdfdict).

createPDFList(directorypath) returns a list of python dictionaries
representing all of the reports from the supplied directory. That is, a list
in which each object is one of the dictionary objects returned by
getSinglePDFObj(). This can be useful if you'd like to access different pdf
objects from a collection without having to know anything about them.

createPDFDict(directorypath) return a dictionary of python dictionaries
representing all of the reports from the supplied directory. That is, a 
dictionary in which the keys is the OCA of a certain report and the value
for that key is the dictionary generated by running getSinglePDFObj on the
report with that OCA. This can be useful if you'd like to access a specific
pdf object from a collection without having to search through to find it.
 


The general path of the program head through xmlScraper.py is as follows:

input -- the path to a converted xml file, or to a directory of them.

getFileLinesList(input) --> returns listOfLines, extraLines
	responsible for reading the file.
	(extraLines are currently simply put aside).

createLineIndex(listOfLines) --> returns lineDataItemList, lineFieldItemList
	responsible for separating field name items vs. data items.
	Uses the helper function getTextBetweenDelimiters(...).

processLists(dlist, flist, sectionsToGrab, extraLines) --> returns pdfObj
	responsible for moving through pdf sections, matching data and adding
	to the master list of key-value pairs. pdfObj is a dictionary.
	sectionsToGrab is a hard-coded list of 'state' strings used in a state
	machine that keeps track of which section of the pdf is being scraped.
	The appropriate states can be found commented out in xmlScraper.py
	and is reproduced below this paragraph (it is also already hardcoded
	into getSinglePDFObj()).
	Uses the helper function grabRelevantDataAndFields(...) and qsort(...)
	and calls the two important subfunctions pairFieldandData(...) and
	markAppropriateFieldsNull(...).

	sectionsToGrab = sectionNameList = snl
	snl = ["AGENCY_INFO", "ARRESTEE_INFO", "ARREST_INFO", "VEH_INFO", "BOND", "DRUGS", "COMP", "NARRATIVE", "STATUS"]

markAppropriateFieldsNull(flist, section) --> returns kvps
	responsible for quickly filling in key-value pairs with NULL for
	pdf sections that are determined to be totally empty of data.
	Quicker; bypasses many comparisons in pairFieldandData(...).
	Also can be useful for debugging, since every report should have
	some information in certain sections (the charge, the arresting
	officer, the name of the person being arrested, etc.).
	Prints a warning statement if one of these sections is empty.

grabRelevantDataAndFields(dlist, flist, start, end)
		--> returns dlist, flist, tempDlist, tempFlist
	responsible for separating off chunks at a time from the main data
	item lists (corresponding to all the fields and data for one section
	of the pdf), so that the chunks can be evaluated in isolation by
	the pairFieldandData(...) function. dlist and flist are the remainder
	of the main lists, and the templists are the chunks.

The meat of the police report scraper functionality is found within the
pairFieldandData() and pairCheckmarkData() functions, which come next.

pairFieldandData(dlistChunk, flistChunk, state) --> returns kvps (key-value pairs)
	responsible for the bulk of the matching work. This function runs once
	for each section of each report being scraped, with the sections
	defined in the snl list above. the dlist and flist chunks are the
	appropriate chunks for that section as collected by
	grabRelevantFieldsAndData(...).

	The matching algorithm differs based on the section being worked on,
	and this is identified by the state parameter. For every section,
	though, before any matching work is attempted in this function, any
	checkbox fields/checkmark data within the section are addressed via
	the pairCheckmarkData(...) function. This is done first for multiple
	reasons:

	1) The set of fields associated with the checkboxes in a given section
	is often satisfyingly large enough that addressing and removing these
	fields first cuts down on a similarly large number of unnecessary
	comparisons later, during the looping required for general matching
	(which will be described in a moment). For instance, in the vehicle
	info section, there are 20 possible field names, and the single check-
	mark kvp 'Vehicle Status' accounts for 12 of them. All 12 can be removed
	after a match is found, leaving a much easier, smaller subset to
	repeatedly cycle through during general matching.
	
	2) Checkmark data is not consistently placed relative to the field
	name describing it. For instance, most checkboxes are situated to the
	left of the field they indicate, but in the Comp section they are
	situated to the right. Thus it is better to explicitly account for
	them first, rather than raise extra exceptions later.

	3) Checkmarks are represented as simply the character 'X' in the file,
	and similarly in the data item list. This makes them ambiguous as far
	as what they refer to when viewed in isolation, but also makes them
	easy to filter out.
	
	see the notes on the pairCheckmarkData(...) function below for details
	on specific matching methods for checkmarks by section.

	Back to pairFieldandData: after the checkmark data is stripped, the
	function enters an if-else ladder that switches based on the 'state'
	string parameter. Only one branch of the ladder runs on any one
	function call; the function is repeatedly called by processLists(...)
	with different values of the state parameter. Specific details of
	the matching algorithms for each section follow:

	AGENCY_INFO

	Agency_Info has the important distinction of containing the OCA field,
	which is tracked in a global variable for convenience in accessibility
	and debugging. It is also home to a strange but apparently common
	problem with the ORI field, in which the data of the Date/Time Arrested
	field is stored in the same file line as the ORI data. As a result all
	three of these fields are tracked, by recording the index where they
	can be found in the list of key-value pairs, so that after all matching
	is done for the section they can be easily accessed again, separated,
	and/or manipulated, as necessary.

	Besides those points, Agency_Info is a good example of the generic
	matching algorithm. This loop structure can be found in multiple other
	sections so it serves well to describe it now. The term 'generic
	matching algorithm' when used in the rest of this doc will refer to
	this process:

	Recall that at this point we have two list chunks: one of data items
	and one of field items (dlistChunk and flistChunk), all within a
	certain section, and with the checkmark-related fields and data
	removed. The generic-matching loop structure begins with the initiation
	of a while loop which will terminate when flistChunk no longer contains
	any items. One run of the while loop compares the coordinates of the
	field at flistChunk[0] with the coordinates of each item in dlistChunk.
	If a match is found, the match is recorded in the list of key-value
	pairs and both the data item and the field item are popped from their
	respective lists. We then break out of the for loop going through the
	data items and move on to the next field item to compare. The field
	item being compared is always at flistChunk[0]. If a match is not found
	after looking through all the data items, then the no-match is recorded
	in the list of key-value pairs by popping the field name off of
	flistChunk and appending it to kvps with the data as 'NULL'.
	
	Thus either way, match or no match, an item is always removed from
	flistChunk on each pass through the loop, ensuring that it will
	eventually terminate. By the time there are no fields left, all of
	the data should have been matched, unless the positioning of form
	elements is vastly different than what is consistently expected.
	
	A successful match is determined by a margin-of-error style check;
	there is a built-in, editable tolerance for each section or even row
	within a section that determines the allowable pixel offset of a data
	or field item from its 'expected' position. Expected positions and the
	current tolerances are hardcoded in the program based on manual
	inspection of a number of converted xml files. They seem to be quite
	accurate but some further tinkering may be necessary as more reports
	are added to the database.
	
	The math of the matching uses the absolute values of two difference
	calculations on the two top coords and two left coords of a pair of
	data and field items, for each data and field item. The absolute
	values must be less than the acceptable tolerance or margin of error.
	
	Specifically, the calculation on the left coordinates requires only a
	simple subtraction -- essentially checking if the data is aligned
	directly below the field name in question, and not offset questionably
	too far to the left or right. if f[1] is the left coord
	of the field and d[1] the left coord of the data, the check is:

		abs(f[1]-d[1]) < moe

	where 'moe' stands for 'margin of error', the number of pixels it can
	be off by. moe is generally 4-6 for exact matching and 10-30 for
	larger feild matching. In the case of Agency_Info, all of the non-
	checkbox fields are fairly wide, so moe is 30 for the left coord.
	
	The calculation on the top coordinate is slightly more complicated,
	but not much; it's the same subtraction, but before performing it,
	we add a positive constant offset to the value of the field item's
	top coordinate. This allows us to match keys and values based solely
	on position (a matching pair of field and data will have approximately
	the same coordinates). if f[0] is the top coord of the field and d[0]
	is the top coord of the data, the check is:

		abs((f[0]+offset)-d[0]) < moe

	where 'moe' is 'margin of error' as before. The value of the offset
	as well as that of the moe can be specified individually for each
	section, and the current values in the code are again the result of
	manual inspection and testing across various agencies that had
	slightly different form element positions. For Agency_Info, the value
	of the offset is 20, and the moe for the top coordinates is 10.

	ARRESTEE_INFO

	There are numerous problem spots for the generic matching algorithm
	in the Arrestee_Info section.
